{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from datasets import Dataset\n",
    "    from transformers import AutoTokenizer\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    # # Set CUDA_VISIBLE_DEVICES to -1 to use CPU only\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"part_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 344277 examples [00:05, 63754.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Ive noticed that several deviations from mandatory reporting standards therefore the data in my report can not be accurate and complete! With that said, I deny your allegations and I challenge you to provide proof that you have the right to report this incomplete negative information on my credit report. \\n\\nPer 15 U.S.C 1681e Whenever a consumer reporting agency prepares a consumer report it shall follow reasonable procedures to assure maximum possible accuracy of the information concerning the individual about whom the report relates. \\n\\nAccording to 12 CFR Part 1022 ( Regulation V ) requires furnishers to : Furnish information concerning accounts or other relationships with a consumer that has integrity. \\nFurnish information about accounts or other relationships with a consumer that is accurate. \\nC - Conduct reasonable investigations of consumer disputes According to XXXX, in order to make a lawful report, you must follow the Metro 2 compliance standards. \\n\\nAlso as stated in the XXXX Any Deviation From These Standards Jeopardizes The Integrity of The Data. This means that any mistakes in reporting claims can make the information less accurate and it affects the accuracy of any federally regulated reports. \\n\\nTherefore, I am invoking my rights to challenge these inaccurate and non-compliant items. Neglecting my demand will only prove that you DO NOT care about my rights as a consumer. Its NOT FAIR for you to report information that isnt confirmed to be ethical and meets the standards of certified reporting. Again, I am challenging these claims and asking for proof of the proper reporting standards. \\n\\nYou are violating my rights! I have listed the concerns about your reporting in this document. It's not complete and does not meet my standards according to law, you must certify all aspects of required FCRA/ Metro 2 compliance which is done truthfully, correctly ( in a timely manner ), completely AND accurately as well - or else I would have to escalate these issues to the CFPB , FTC and my Attorney General.\", \"I reviewed my consumer reports and noticed that I had ( Insert Number of Late Payments ) late payments on an account that I was never late for. The consumer reporting agencies have assumed a vital role and have a responsibility to report consumer information to the best of their ability with maximum accuracy. I have never been late on payments for the accounts that I have included screenshot inside this complaint in on dates of [ XX/XX/22 ]. Please investigate and provide proof with statements showing that these payments were late. If proof can't be provided I demand these payments be updated to paid as agreed on time as they should be due to the fact i was never late. Thanks! Please see the attached screenshot.\", 'I have been trying to get assistance from my mortgage company for months to a year now. I am just trying to get my mortgage caught up, so I can start paying again. I fell behind due to loss in income. Every time I send in the items they ask for, they turn around and ask for more items. I sent in the final list of items they needed on  XX/XX/year>. We called back and there was nothing needed. \\n\\nNext thing I know they are asking me for a HAND WRITTEN document and all of the same items again.\\n\\nXXXX days later, a man showed up at my doorstep stating there was a sale date for XX/XX/year>. \\n\\nI have every fax and email confirmation that has been sent to New American Funding. They just keep asking me for new things every time I send them what they need and then they assessed a sale date after I have done EVERYTHING they have asked me to do.\\n\\nI am attaching the documents to this complaint again for review in hopes to stop this impending sale date.\\n\\nAll I am asking for is them to review this correctly. I am ready to resume making my payments again, I just need help getting caught up.', 'Please see identity theft report. I have consistently disputed fraudulent accounts that are results from identity theft on my credit profile with all 3 credit bureaus and they have ignored my verified claims and are unlawfully and illegally reporting fraudulent accounts on my credit profile.', 'In the mid XXXX  \\'s I had a checking account with US Bank. I closed this account with a negative balance and paid this balance in FULL. I have proof of payment both in writing and bank statement showing the payment. Over the last decade I have had multiple agencies reach out to me about pending civil litigation on behalf of US Bank to collect on this debt. the balance now being claimed I owe is double the original balance due to \" fees and penalties \\'\\'. I have had to prove on at least 5 separate accounts most recently today that I have paid this debt. I feel at this point I can be sued for debt paid if I ever happen to lose this receipt. I also do not understand why this debt is still being sold to agencies when it has been paid, At this point I do not know what my options are.']\n",
      "['Improper use of your report', 'Incorrect information on your report', 'Struggling to pay mortgage', 'Incorrect information on your report', 'Attempts to collect debt not owed']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"Consumer complaint narrative\"][:5])\n",
    "print(dataset[\"Issue\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "[86, 4408, 663, 28, 8, 4506, 6529, 3750, 53, 1810, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 6288, 1713, 3, 4, 4, 4, 4, 6, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 6288, 1713, 3, 4, 4, 4, 4, 6, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 6288, 1713, 3, 226, 19230, 19230, 3, 4, 4, 4, 4, 6, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 6288, 1713, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 6, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 6, 11, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 6288, 1713, 3, 19230, 19230, 19230, 3, 4, 4, 4, 4, 6, 65, 15108, 26, 82, 2166, 5, 627, 412, 5, 134, 5, 254, 898, 4959, 1375, 1640, 357, 71, 5, 1323, 27, 43, 8, 269, 12, 4570, 5, 627, 412, 5, 134, 5, 254, 898, 4959, 5568, 1640, 591, 71, 5568, 204, 3, 10, 94, 92, 2315, 3, 9, 3733, 5099, 3193, 54, 59, 26808, 3, 9, 905, 406, 82, 1545, 3909, 100, 998, 127, 65, 118, 16, 1223, 12374, 13, 82, 2166, 57, 5099, 27801, 251, 30, 82, 998, 934, 5, 1]\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(example['Consumer complaint narrative'])\n",
    "for key in tokenized_text:\n",
    "    print(key)\n",
    "    print(tokenized_text[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Prepends the string \"summarize: \" to each document in the 'text' field of the input examples.\n",
    "    # This is done to instruct the T5 model on the task it needs to perform, which in this case is summarization.\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"Consumer complaint narrative\"]]\n",
    "\n",
    "    # Tokenizes the prepended input texts to convert them into a format that can be fed into the T5 model.\n",
    "    # Sets a maximum token length of 1024, and truncates any text longer than this limit.\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Tokenizes the 'summary' field of the input examples to prepare the target labels for the summarization task.\n",
    "    # Sets a maximum token length of 128, and truncates any text longer than this limit.\n",
    "    labels = tokenizer(text_target=examples[\"Issue\"], max_length=128, truncation=True)\n",
    "\n",
    "    # Assigns the tokenized labels to the 'labels' field of model_inputs.\n",
    "    # The 'labels' field is used during training to calculate the loss and guide model learning.\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Returns the prepared inputs and labels as a single dictionary, ready for training.\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 275421/275421 [01:32<00:00, 2966.80 examples/s]\n",
      "Map: 100%|██████████| 68856/68856 [00:23<00:00, 2962.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_billsum = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Requested information to pay off homeXXXX Received first incorrect payoff statement-XXXX. This was incorrect because of the amount of interest due, an item called \" Funds owed by borrower \\'\\', and because the amount of taxes paid by the lender. The lender is also the home builder and paid taxes on the property but paid them late which resulted in late fees. \\n\\nSince XX/XX/XXXX we have received multiple payoff statements. United Built Homes ( UBH ) has not responded to requests for an explanation of Funds owed by Borrower, has not stated if they will pay for late fees due to their late payment on property taxes, and on the most recent payoff statement, UBH has included insurance and property taxes with these disbursement dates : property taxes XX/XX/XXXX and insurance XX/XX/XXXX. We were told in email to disregard the insurance, but the most recent payoff statement does not reflect this. I have asked for documentation from UBH that the current year \\'s taxes have been paid and have not received this, and the county website ( last time I checked ) does not show these taxes as having been paid.\\n\\nIt is documented in email that I have requested accurate payoff statements multiple times. Further, I have sent two wire transfers dated XX/XX/XXXX and XX/XX/XXXX that when combined amount to the cost of the home with taxes that were paid for XXXX minus the late fee incurred by UBH, though UBH is including this amount in the payoff statement.\\n\\nUBH is currently stating that they are refusing these wire transfers, but I confirmed, through speaking with my bank, that as of today those transfers were not rejected or returned. That money is no longer in my account and has not been since the dates listed above. However, UBH has not updated their payoff statement and we consider the most recent payoff statement to be inaccurate and to include interest and charges that we either have not received explanation for from UBH or is simply inaccurate.\\n\\nFurther, we do not consider the work on our home to be complete as outlined in our agreement with UBH and communication with UBH employees.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_billsum['test'][0]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trouble during payment process'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_billsum['test'][0]['Issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpacks the evaluation predictions tuple into predictions and labels.\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decodes the tokenized predictions back to text, skipping any special tokens (e.g., padding tokens).\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replaces any -100 values in labels with the tokenizer's pad_token_id.\n",
    "    # This is done because -100 is often used to ignore certain tokens when calculating the loss during training.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decodes the tokenized labels back to text, skipping any special tokens (e.g., padding tokens).\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Computes the ROUGE metric between the decoded predictions and decoded labels.\n",
    "    # The use_stemmer parameter enables stemming, which reduces words to their root form before comparison.\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Calculates the length of each prediction by counting the non-padding tokens.\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "\n",
    "    # Computes the mean length of the predictions and adds it to the result dictionary under the key \"gen_len\".\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Rounds each value in the result dictionary to 4 decimal places for cleaner output, and returns the result.\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_fine_tuned_t5_small_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110172 [02:12<?, ?it/s]\n",
      "  0%|          | 3/110172 [00:26<250:45:03,  8.19s/it]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 745.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\Barclays\\t5-trans.ipynb Cell 21\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Barclays/t5-trans.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1625\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1626\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1627\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1628\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1629\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2913\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:1999\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1997\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1998\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1999\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2001\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 745.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device name: NVIDIA GeForce GTX 1650\n",
      "CUDA device name at index 0: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check the name of the current CUDA device\n",
    "device_name = torch.cuda.get_device_name()\n",
    "print(\"Current CUDA device name:\", device_name)\n",
    "\n",
    "# Alternatively, you can specify the device index\n",
    "device_index = 0  # Example: Check the name of device at index 0\n",
    "device_name_at_index = torch.cuda.get_device_name(device_index)\n",
    "print(f\"CUDA device name at index {device_index}:\", device_name_at_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
